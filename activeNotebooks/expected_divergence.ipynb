{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True) # Seaborn desaturates matplotlib colors (this is useful if you use both seaborn and basic matplot lib and want colors to be consistent)\n",
    "import scipy.stats as stats\n",
    "sns.set(palette='husl', context = 'talk', style='white', font_scale=1)\n",
    "\n",
    "import os\n",
    "import scikit_posthocs as sp\n",
    "from pybedtools import BedTool\n",
    "import pybedtools\n",
    "from upsetplot import plot, from_indicators\n",
    "import pysam\n",
    "import json\n",
    "import re\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 15:43:57.790272: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1' ### run on CPU\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "if tf.__version__[0] == '1':\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "from basenji import dataset, dna_io, seqnn\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"/\".join(os.getcwd().split(\"/\")) # base directory level\n",
    "\n",
    "\n",
    "#Wynton\n",
    "BIN_PATH = os.path.join(BASE_PATH, \"bin\")  # where my scripts live\n",
    "DATA_PATH = os.path.join(BASE_PATH, \"data\")  # where I dump new data \n",
    "RESULTS_PATH = os.path.join(BASE_PATH, \"results\")  # where I analyze results\n",
    "\n",
    "SRC_PATH = os.path.join(BASE_PATH, \"src\")  # where any packages needed to run analyses live. I haven't started structuring things this way yet. \n",
    "\n",
    "COMP_PATH = os.path.join(DATA_PATH,\"pairwise/hsmrca\")\n",
    "COMP_PATH = os.path.join(DATA_PATH,\"pairwise/reference\")\n",
    "\n",
    "\n",
    "# Local\n",
    "\n",
    "# DATA_PATH = os.path.join(BASE_PATH, \"../../../dowloads/\")  # where I dump new data\n",
    "# COMP_PATH = os.path.join(DATA_PATH,\"1KGvsHSMRCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wynton/group/capra/projects/modern_human_3Dgenome'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTA_PATH = os.path.join(DATA_PATH, \"genomes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = '%s/basenji/manuscripts/akita/' % BIN_PATH\n",
    "AKITA_PARAMS = 'params.json'\n",
    "\n",
    "MODEL = 'model_best.h5'\n",
    "\n",
    "TARGETS = \"targets.txt\"\n",
    "\n",
    "STATS = \"statistics.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load params and model\n",
      "WARNING:tensorflow:From /wynton/home/capra/egilbertson/.conda/envs/modern3d/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 15:44:27.689532: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sequence (InputLayer)          [(None, 1048576, 4)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " stochastic_reverse_complement   ((None, 1048576, 4)  0          ['sequence[0][0]']               \n",
      " (StochasticReverseComplement)  , ())                                                             \n",
      "                                                                                                  \n",
      " stochastic_shift (StochasticSh  (None, 1048576, 4)  0           ['stochastic_reverse_complement[0\n",
      " ift)                                                            ][0]']                           \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 1048576, 4)   0           ['stochastic_shift[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 1048576, 96)  4224        ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 1048576, 96)  384        ['conv1d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 524288, 96)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 524288, 96)   0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 524288, 96)   46080       ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 524288, 96)  384         ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 262144, 96)  0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 262144, 96)   0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 262144, 96)   46080       ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 262144, 96)  384         ['conv1d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 131072, 96)  0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)                 (None, 131072, 96)   0           ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 131072, 96)   46080       ['re_lu_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 131072, 96)  384         ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 65536, 96)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)                 (None, 65536, 96)    0           ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 65536, 96)    46080       ['re_lu_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 65536, 96)   384         ['conv1d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 32768, 96)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)                 (None, 32768, 96)    0           ['max_pooling1d_4[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 32768, 96)    46080       ['re_lu_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32768, 96)   384         ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 16384, 96)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)                 (None, 16384, 96)    0           ['max_pooling1d_5[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 16384, 96)    46080       ['re_lu_6[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 16384, 96)   384         ['conv1d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPooling1D)  (None, 8192, 96)    0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)                 (None, 8192, 96)     0           ['max_pooling1d_6[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 8192, 96)     46080       ['re_lu_7[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 8192, 96)    384         ['conv1d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPooling1D)  (None, 4096, 96)    0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)                 (None, 4096, 96)     0           ['max_pooling1d_7[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 4096, 96)     46080       ['re_lu_8[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 4096, 96)    384         ['conv1d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPooling1D)  (None, 2048, 96)    0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)                 (None, 2048, 96)     0           ['max_pooling1d_8[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 2048, 96)     46080       ['re_lu_9[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 2048, 96)    384         ['conv1d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPooling1D)  (None, 1024, 96)    0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)                (None, 1024, 96)     0           ['max_pooling1d_9[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 1024, 96)     46080       ['re_lu_10[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 1024, 96)    384         ['conv1d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling1d_10 (MaxPooling1D  (None, 512, 96)     0           ['batch_normalization_10[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)                (None, 512, 96)      0           ['max_pooling1d_10[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 512, 48)      13824       ['re_lu_11[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 512, 48)     192         ['conv1d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)                (None, 512, 48)      0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 512, 96)      4608        ['re_lu_12[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 512, 96)     384         ['conv1d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512, 96)      0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 512, 96)      0           ['max_pooling1d_10[0][0]',       \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)                (None, 512, 96)      0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 512, 48)      13824       ['re_lu_13[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 512, 48)     192         ['conv1d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)                (None, 512, 48)      0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 512, 96)      4608        ['re_lu_14[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 512, 96)     384         ['conv1d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 512, 96)      0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 512, 96)      0           ['add[0][0]',                    \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)                (None, 512, 96)      0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 512, 48)      13824       ['re_lu_15[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 512, 48)     192         ['conv1d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)                (None, 512, 48)      0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)             (None, 512, 96)      4608        ['re_lu_16[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 512, 96)     384         ['conv1d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 512, 96)      0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 512, 96)      0           ['add_1[0][0]',                  \n",
      "                                                                  'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)                (None, 512, 96)      0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)             (None, 512, 48)      13824       ['re_lu_17[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 512, 48)     192         ['conv1d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)                (None, 512, 48)      0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)             (None, 512, 96)      4608        ['re_lu_18[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 512, 96)     384         ['conv1d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 512, 96)      0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 512, 96)      0           ['add_2[0][0]',                  \n",
      "                                                                  'dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)                (None, 512, 96)      0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)             (None, 512, 48)      13824       ['re_lu_19[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 512, 48)     192         ['conv1d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)                (None, 512, 48)      0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)             (None, 512, 96)      4608        ['re_lu_20[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 512, 96)     384         ['conv1d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 512, 96)      0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 512, 96)      0           ['add_3[0][0]',                  \n",
      "                                                                  'dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)                (None, 512, 96)      0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)             (None, 512, 48)      13824       ['re_lu_21[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 512, 48)     192         ['conv1d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)                (None, 512, 48)      0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)             (None, 512, 96)      4608        ['re_lu_22[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 512, 96)     384         ['conv1d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 512, 96)      0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 512, 96)      0           ['add_4[0][0]',                  \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)                (None, 512, 96)      0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)             (None, 512, 48)      13824       ['re_lu_23[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 512, 48)     192         ['conv1d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)                (None, 512, 48)      0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)             (None, 512, 96)      4608        ['re_lu_24[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 512, 96)     384         ['conv1d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 512, 96)      0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 512, 96)      0           ['add_5[0][0]',                  \n",
      "                                                                  'dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)                (None, 512, 96)      0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)             (None, 512, 48)      13824       ['re_lu_25[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 512, 48)     192         ['conv1d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)                (None, 512, 48)      0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)             (None, 512, 96)      4608        ['re_lu_26[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 512, 96)     384         ['conv1d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 512, 96)      0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 512, 96)      0           ['add_6[0][0]',                  \n",
      "                                                                  'dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)                (None, 512, 96)      0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)             (None, 512, 64)      30720       ['re_lu_27[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 512, 64)     256         ['conv1d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " re_lu_28 (ReLU)                (None, 512, 64)      0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " one_to_two (OneToTwo)          (None, 512, 512, 64  0           ['re_lu_28[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concat_dist2d (ConcatDist2D)   (None, 512, 512, 65  0           ['one_to_two[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_29 (ReLU)                (None, 512, 512, 65  0           ['concat_dist2d[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 512, 512, 48  28080       ['re_lu_29[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 512, 512, 48  192        ['conv2d[0][0]']                 \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " symmetrize2d (Symmetrize2D)    (None, 512, 512, 48  0           ['batch_normalization_28[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_30 (ReLU)                (None, 512, 512, 48  0           ['symmetrize2d[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 512, 512, 24  10368       ['re_lu_30[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 512, 512, 24  96         ['conv2d_1[0][0]']               \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_31 (ReLU)                (None, 512, 512, 24  0           ['batch_normalization_29[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 512, 512, 48  1152        ['re_lu_31[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 512, 512, 48  192        ['conv2d_2[0][0]']               \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 512, 512, 48  0           ['batch_normalization_30[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 512, 512, 48  0           ['symmetrize2d[0][0]',           \n",
      "                                )                                 'dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " symmetrize2d_1 (Symmetrize2D)  (None, 512, 512, 48  0           ['add_8[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_32 (ReLU)                (None, 512, 512, 48  0           ['symmetrize2d_1[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 512, 512, 24  10368       ['re_lu_32[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 512, 512, 24  96         ['conv2d_3[0][0]']               \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_33 (ReLU)                (None, 512, 512, 24  0           ['batch_normalization_31[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 512, 512, 48  1152        ['re_lu_33[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 512, 512, 48  192        ['conv2d_4[0][0]']               \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 512, 512, 48  0           ['batch_normalization_32[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 512, 512, 48  0           ['symmetrize2d_1[0][0]',         \n",
      "                                )                                 'dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " symmetrize2d_2 (Symmetrize2D)  (None, 512, 512, 48  0           ['add_9[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_34 (ReLU)                (None, 512, 512, 48  0           ['symmetrize2d_2[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 512, 512, 24  10368       ['re_lu_34[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 512, 512, 24  96         ['conv2d_5[0][0]']               \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_35 (ReLU)                (None, 512, 512, 24  0           ['batch_normalization_33[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 512, 512, 48  1152        ['re_lu_35[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 512, 512, 48  192        ['conv2d_6[0][0]']               \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 512, 512, 48  0           ['batch_normalization_34[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 512, 512, 48  0           ['symmetrize2d_2[0][0]',         \n",
      "                                )                                 'dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " symmetrize2d_3 (Symmetrize2D)  (None, 512, 512, 48  0           ['add_10[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_36 (ReLU)                (None, 512, 512, 48  0           ['symmetrize2d_3[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 512, 512, 24  10368       ['re_lu_36[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 512, 512, 24  96         ['conv2d_7[0][0]']               \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_37 (ReLU)                (None, 512, 512, 24  0           ['batch_normalization_35[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 512, 512, 48  1152        ['re_lu_37[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 512, 512, 48  192        ['conv2d_8[0][0]']               \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 512, 512, 48  0           ['batch_normalization_36[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 512, 512, 48  0           ['symmetrize2d_3[0][0]',         \n",
      "                                )                                 'dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " symmetrize2d_4 (Symmetrize2D)  (None, 512, 512, 48  0           ['add_11[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_38 (ReLU)                (None, 512, 512, 48  0           ['symmetrize2d_4[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 512, 512, 24  10368       ['re_lu_38[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 512, 512, 24  96         ['conv2d_9[0][0]']               \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_39 (ReLU)                (None, 512, 512, 24  0           ['batch_normalization_37[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 512, 512, 48  1152        ['re_lu_39[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 512, 512, 48  192        ['conv2d_10[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 512, 512, 48  0           ['batch_normalization_38[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 512, 512, 48  0           ['symmetrize2d_4[0][0]',         \n",
      "                                )                                 'dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " symmetrize2d_5 (Symmetrize2D)  (None, 512, 512, 48  0           ['add_12[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_40 (ReLU)                (None, 512, 512, 48  0           ['symmetrize2d_5[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 512, 512, 24  10368       ['re_lu_40[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 512, 512, 24  96         ['conv2d_11[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " re_lu_41 (ReLU)                (None, 512, 512, 24  0           ['batch_normalization_39[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 512, 512, 48  1152        ['re_lu_41[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 512, 512, 48  192        ['conv2d_12[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 512, 512, 48  0           ['batch_normalization_40[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 512, 512, 48  0           ['symmetrize2d_5[0][0]',         \n",
      "                                )                                 'dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " symmetrize2d_6 (Symmetrize2D)  (None, 512, 512, 48  0           ['add_13[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " cropping2d (Cropping2D)        (None, 448, 448, 48  0           ['symmetrize2d_6[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " upper_tri (UpperTri)           (None, 99681, 48)    0           ['cropping2d[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 99681, 5)     245         ['upper_tri[0][0]']              \n",
      "                                                                                                  \n",
      " switch_reverse_triu (SwitchRev  (None, 99681, 5)    0           ['dense[0][0]',                  \n",
      " erseTriu)                                                        'stochastic_reverse_complement[0\n",
      "                                                                 ][1]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 751,653\n",
      "Trainable params: 746,149\n",
      "Non-trainable params: 5,504\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "model_strides [2048]\n",
      "target_lengths [99681]\n",
      "target_crops [-49585]\n",
      "restore model\n"
     ]
    }
   ],
   "source": [
    "#Setting up Akita model\n",
    "### load params, specify model ###\n",
    "print('load params and model')\n",
    "params_file = os.path.join(MODEL_DIR, AKITA_PARAMS)\n",
    "model_file  = os.path.join(MODEL_DIR, MODEL)\n",
    "with open(params_file) as params_open:\n",
    "    params = json.load(params_open)\n",
    "    params_model = params['model']\n",
    "    params_train = params['train']\n",
    "\n",
    "seqnn_model = seqnn.SeqNN(params_model)\n",
    "\n",
    "### restore model ###\n",
    "print('restore model')\n",
    "seqnn_model.restore(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data paramenters\n",
      "flattened representation length: 99681\n",
      "symmetrix matrix size: (448,448)\n"
     ]
    }
   ],
   "source": [
    "print('read data paramenters')\n",
    "data_stats_file = os.path.join(MODEL_DIR, \"data/%s\" % STATS)\n",
    "with open(data_stats_file) as data_stats_open:\n",
    "    data_stats = json.load(data_stats_open)\n",
    "seq_length = data_stats['seq_length']\n",
    "target_length = data_stats['target_length']\n",
    "hic_diags =  data_stats['diagonal_offset']\n",
    "target_crop = data_stats['crop_bp'] // data_stats['pool_width']\n",
    "target_length1 = data_stats['seq_length'] // data_stats['pool_width']\n",
    "\n",
    "\n",
    "target_length1_cropped = target_length1 - 2*target_crop\n",
    "print('flattened representation length:', target_length)\n",
    "print('symmetrix matrix size:', '('+str(target_length1_cropped)+','+str(target_length1_cropped)+')')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAkitaPreds(seq):\n",
    "    print('run predictions')\n",
    "    if len(seq) != 2**20: raise ValueError('len(seq) != seq_length')\n",
    "    seq_1hot = dna_io.dna_1hot(seq)\n",
    "    test_pred_from_seq = seqnn_model.model.predict(np.expand_dims(seq_1hot,0))\n",
    "    return test_pred_from_seq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter plot by windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ancestral 3D dataframes\n",
    "anc_spear = pd.read_table('%s/comp_tables/anc_window_spearman.csv' % RESULTS_PATH\n",
    "                    , sep=',', header=[0,1,2,3], index_col=[0,1])\n",
    "anc_div = 1-anc_spear\n",
    "windows_to_keep = anc_div.index\n",
    "\n",
    "anc = pd.read_table('%s/comp_tables/anc_genomewide_averages.csv' % RESULTS_PATH,\n",
    "                    sep=',', index_col=0)\n",
    "drops = anc[np.isnan(anc.genome_avg_mse)].index\n",
    "anc = anc.drop(index=drops)\n",
    "anc['divergence'] = 1-anc.genome_avg_spearman\n",
    "windows_to_keep.to_frame(index=False).to_csv('%s/intermediates/windows_to_keep.csv' % DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">AFR</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">SAS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">ACB</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">STU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">female</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">male</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>HG01880</th>\n",
       "      <th>HG01883</th>\n",
       "      <th>HG01886</th>\n",
       "      <th>HG01889</th>\n",
       "      <th>HG01894</th>\n",
       "      <th>HG01896</th>\n",
       "      <th>HG01915</th>\n",
       "      <th>HG01956</th>\n",
       "      <th>HG01958</th>\n",
       "      <th>HG01985</th>\n",
       "      <th>...</th>\n",
       "      <th>HG03998</th>\n",
       "      <th>HG03999</th>\n",
       "      <th>HG04003</th>\n",
       "      <th>HG04006</th>\n",
       "      <th>HG04033</th>\n",
       "      <th>HG04039</th>\n",
       "      <th>HG04100</th>\n",
       "      <th>HG04107</th>\n",
       "      <th>HG04210</th>\n",
       "      <th>HG04229</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr</th>\n",
       "      <th>windowStartPos</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">chr1</th>\n",
       "      <th>1048576</th>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.002145</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.001096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572864</th>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.001311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3145728</th>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.002624</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>0.003151</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.005576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3670016</th>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.004503</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.006236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005861</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>0.004933</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>0.004428</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.006116</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.005439</td>\n",
       "      <td>0.005394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194304</th>\n",
       "      <td>0.008681</td>\n",
       "      <td>0.007497</td>\n",
       "      <td>0.004502</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.011335</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>0.010622</td>\n",
       "      <td>0.007037</td>\n",
       "      <td>0.014604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>0.011672</td>\n",
       "      <td>0.013004</td>\n",
       "      <td>0.005318</td>\n",
       "      <td>0.008290</td>\n",
       "      <td>0.008384</td>\n",
       "      <td>0.010748</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.011217</td>\n",
       "      <td>0.009877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">chr22</th>\n",
       "      <th>46661632</th>\n",
       "      <td>0.007145</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.008396</td>\n",
       "      <td>0.004389</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.006965</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.005317</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.006143</td>\n",
       "      <td>0.002893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47185920</th>\n",
       "      <td>0.027227</td>\n",
       "      <td>0.006835</td>\n",
       "      <td>0.003085</td>\n",
       "      <td>0.035391</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.002898</td>\n",
       "      <td>0.017533</td>\n",
       "      <td>0.005524</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.019010</td>\n",
       "      <td>0.149662</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>0.005283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47710208</th>\n",
       "      <td>0.039303</td>\n",
       "      <td>0.045497</td>\n",
       "      <td>0.065454</td>\n",
       "      <td>0.123312</td>\n",
       "      <td>0.028221</td>\n",
       "      <td>0.040250</td>\n",
       "      <td>0.046180</td>\n",
       "      <td>0.030846</td>\n",
       "      <td>0.038820</td>\n",
       "      <td>0.033115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037299</td>\n",
       "      <td>0.045959</td>\n",
       "      <td>0.036592</td>\n",
       "      <td>0.034629</td>\n",
       "      <td>0.043876</td>\n",
       "      <td>0.036689</td>\n",
       "      <td>0.040327</td>\n",
       "      <td>0.087711</td>\n",
       "      <td>0.034104</td>\n",
       "      <td>0.045514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48234496</th>\n",
       "      <td>0.183482</td>\n",
       "      <td>0.189969</td>\n",
       "      <td>0.241011</td>\n",
       "      <td>0.325763</td>\n",
       "      <td>0.195117</td>\n",
       "      <td>0.174392</td>\n",
       "      <td>0.179873</td>\n",
       "      <td>0.198540</td>\n",
       "      <td>0.168733</td>\n",
       "      <td>0.161755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166427</td>\n",
       "      <td>0.189953</td>\n",
       "      <td>0.157409</td>\n",
       "      <td>0.160880</td>\n",
       "      <td>0.186454</td>\n",
       "      <td>0.157263</td>\n",
       "      <td>0.187399</td>\n",
       "      <td>0.171310</td>\n",
       "      <td>0.133338</td>\n",
       "      <td>0.183812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48758784</th>\n",
       "      <td>0.005628</td>\n",
       "      <td>0.014172</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.015048</td>\n",
       "      <td>0.012794</td>\n",
       "      <td>0.003188</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>0.012890</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.013330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005060</td>\n",
       "      <td>0.005483</td>\n",
       "      <td>0.003194</td>\n",
       "      <td>0.004617</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>0.005568</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>0.004660</td>\n",
       "      <td>0.005613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4873 rows  2457 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           AFR                                          \\\n",
       "                           ACB                                           \n",
       "                        female                                           \n",
       "                       HG01880   HG01883   HG01886   HG01889   HG01894   \n",
       "chr   windowStartPos                                                     \n",
       "chr1  1048576         0.000982  0.001551  0.001609  0.001618  0.003944   \n",
       "      1572864         0.001445  0.001657  0.001909  0.001959  0.004272   \n",
       "      3145728         0.001283  0.000683  0.002624  0.002458  0.002041   \n",
       "      3670016         0.005119  0.004878  0.001901  0.002729  0.004874   \n",
       "      4194304         0.008681  0.007497  0.004502  0.004731  0.011335   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "chr22 46661632        0.007145  0.001892  0.002877  0.008396  0.004389   \n",
       "      47185920        0.027227  0.006835  0.003085  0.035391  0.003696   \n",
       "      47710208        0.039303  0.045497  0.065454  0.123312  0.028221   \n",
       "      48234496        0.183482  0.189969  0.241011  0.325763  0.195117   \n",
       "      48758784        0.005628  0.014172  0.002837  0.015048  0.012794   \n",
       "\n",
       "                                                                        ...  \\\n",
       "                                                                        ...   \n",
       "                                                                        ...   \n",
       "                       HG01896   HG01915   HG01956   HG01958   HG01985  ...   \n",
       "chr   windowStartPos                                                    ...   \n",
       "chr1  1048576         0.002145  0.001217  0.000658  0.002693  0.002143  ...   \n",
       "      1572864         0.002573  0.001838  0.002096  0.006768  0.002917  ...   \n",
       "      3145728         0.002661  0.000521  0.002490  0.003151  0.002294  ...   \n",
       "      3670016         0.001535  0.004503  0.004706  0.004062  0.006236  ...   \n",
       "      4194304         0.006836  0.007988  0.010622  0.007037  0.014604  ...   \n",
       "...                        ...       ...       ...       ...       ...  ...   \n",
       "chr22 46661632        0.002007  0.003275  0.006965  0.002858  0.006226  ...   \n",
       "      47185920        0.007332  0.002898  0.017533  0.005524  0.002682  ...   \n",
       "      47710208        0.040250  0.046180  0.030846  0.038820  0.033115  ...   \n",
       "      48234496        0.174392  0.179873  0.198540  0.168733  0.161755  ...   \n",
       "      48758784        0.003188  0.005283  0.012890  0.003300  0.013330  ...   \n",
       "\n",
       "                           SAS                                          \\\n",
       "                           STU                                           \n",
       "                          male                                           \n",
       "                       HG03998   HG03999   HG04003   HG04006   HG04033   \n",
       "chr   windowStartPos                                                     \n",
       "chr1  1048576         0.000387  0.000713  0.000919  0.000718  0.000841   \n",
       "      1572864         0.001048  0.000703  0.001067  0.000496  0.001208   \n",
       "      3145728         0.003141  0.004537  0.000520  0.000528  0.001665   \n",
       "      3670016         0.005861  0.006603  0.004933  0.004321  0.004428   \n",
       "      4194304         0.009956  0.011672  0.013004  0.005318  0.008290   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "chr22 46661632        0.003228  0.002667  0.003284  0.002222  0.001290   \n",
       "      47185920        0.007095  0.003824  0.004329  0.003580  0.003119   \n",
       "      47710208        0.037299  0.045959  0.036592  0.034629  0.043876   \n",
       "      48234496        0.166427  0.189953  0.157409  0.160880  0.186454   \n",
       "      48758784        0.005060  0.005483  0.003194  0.004617  0.005598   \n",
       "\n",
       "                                                                        \n",
       "                                                                        \n",
       "                                                                        \n",
       "                       HG04039   HG04100   HG04107   HG04210   HG04229  \n",
       "chr   windowStartPos                                                    \n",
       "chr1  1048576         0.000579  0.000697  0.000996  0.000801  0.001096  \n",
       "      1572864         0.000840  0.001380  0.001535  0.002401  0.001311  \n",
       "      3145728         0.000513  0.000395  0.001547  0.000535  0.005576  \n",
       "      3670016         0.004286  0.006116  0.001554  0.005439  0.005394  \n",
       "      4194304         0.008384  0.010748  0.003171  0.011217  0.009877  \n",
       "...                        ...       ...       ...       ...       ...  \n",
       "chr22 46661632        0.003560  0.005317  0.002408  0.006143  0.002893  \n",
       "      47185920        0.002920  0.019010  0.149662  0.006732  0.005283  \n",
       "      47710208        0.036689  0.040327  0.087711  0.034104  0.045514  \n",
       "      48234496        0.157263  0.187399  0.171310  0.133338  0.183812  \n",
       "      48758784        0.005568  0.007674  0.006719  0.004660  0.005613  \n",
       "\n",
       "[4873 rows x 2457 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anc_div"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and format sequence comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_diff = pd.read_table('%s/comp_tables/anc_window_seq_diff.csv' % RESULTS_PATH, sep=',',\n",
    " header=[0,1,2,3], index_col=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">AFR</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">SAS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">ACB</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">STU</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">female</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">male</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>HG01880</th>\n",
       "      <th>HG01883</th>\n",
       "      <th>HG01886</th>\n",
       "      <th>HG01889</th>\n",
       "      <th>HG01894</th>\n",
       "      <th>HG01896</th>\n",
       "      <th>HG01915</th>\n",
       "      <th>HG01956</th>\n",
       "      <th>HG01958</th>\n",
       "      <th>HG01985</th>\n",
       "      <th>...</th>\n",
       "      <th>HG03998</th>\n",
       "      <th>HG03999</th>\n",
       "      <th>HG04003</th>\n",
       "      <th>HG04006</th>\n",
       "      <th>HG04033</th>\n",
       "      <th>HG04039</th>\n",
       "      <th>HG04100</th>\n",
       "      <th>HG04107</th>\n",
       "      <th>HG04210</th>\n",
       "      <th>HG04229</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chr</th>\n",
       "      <th>windowStartPos</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">chr1</th>\n",
       "      <th>1048576</th>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.002254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.001890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572864</th>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.001741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3145728</th>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.002402</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.002134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3670016</th>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.002371</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002339</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.002237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194304</th>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.002097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">chr22</th>\n",
       "      <th>46661632</th>\n",
       "      <td>0.002152</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47185920</th>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.001973</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.002043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47710208</th>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>0.002611</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.002489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48234496</th>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.003364</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>0.003148</td>\n",
       "      <td>0.003270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.003124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48758784</th>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>0.003144</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.003035</td>\n",
       "      <td>0.003151</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>0.002879</td>\n",
       "      <td>0.002802</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.002911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4873 rows  2457 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           AFR                                          \\\n",
       "                           ACB                                           \n",
       "                        female                                           \n",
       "                       HG01880   HG01883   HG01886   HG01889   HG01894   \n",
       "chr   windowStartPos                                                     \n",
       "chr1  1048576         0.002069  0.002143  0.002264  0.002028  0.002147   \n",
       "      1572864         0.001890  0.001877  0.001849  0.001808  0.001760   \n",
       "      3145728         0.002227  0.002374  0.002298  0.002382  0.002401   \n",
       "      3670016         0.002389  0.002414  0.002371  0.002477  0.002459   \n",
       "      4194304         0.002138  0.002143  0.002124  0.002177  0.002175   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "chr22 46661632        0.002152  0.002144  0.001987  0.002261  0.002272   \n",
       "      47185920        0.002042  0.002110  0.001939  0.002110  0.002114   \n",
       "      47710208        0.002494  0.002572  0.002554  0.002591  0.002642   \n",
       "      48234496        0.003228  0.003226  0.003368  0.003326  0.003364   \n",
       "      48758784        0.003024  0.003116  0.003038  0.003170  0.003144   \n",
       "\n",
       "                                                                        ...  \\\n",
       "                                                                        ...   \n",
       "                                                                        ...   \n",
       "                       HG01896   HG01915   HG01956   HG01958   HG01985  ...   \n",
       "chr   windowStartPos                                                    ...   \n",
       "chr1  1048576         0.002197  0.002199  0.002124  0.002051  0.002254  ...   \n",
       "      1572864         0.001850  0.001861  0.001867  0.001761  0.001828  ...   \n",
       "      3145728         0.002306  0.002377  0.002364  0.002402  0.002195  ...   \n",
       "      3670016         0.002342  0.002415  0.002535  0.002457  0.002362  ...   \n",
       "      4194304         0.002050  0.002172  0.002123  0.002101  0.002054  ...   \n",
       "...                        ...       ...       ...       ...       ...  ...   \n",
       "chr22 46661632        0.002284  0.002222  0.002307  0.002239  0.002378  ...   \n",
       "      47185920        0.002144  0.002131  0.002155  0.002106  0.002221  ...   \n",
       "      47710208        0.002570  0.002561  0.002611  0.002545  0.002628  ...   \n",
       "      48234496        0.003214  0.003247  0.003374  0.003148  0.003270  ...   \n",
       "      48758784        0.003012  0.003035  0.003151  0.002872  0.002978  ...   \n",
       "\n",
       "                           SAS                                          \\\n",
       "                           STU                                           \n",
       "                          male                                           \n",
       "                       HG03998   HG03999   HG04003   HG04006   HG04033   \n",
       "chr   windowStartPos                                                     \n",
       "chr1  1048576         0.001787  0.001892  0.002039  0.001963  0.002067   \n",
       "      1572864         0.001675  0.001752  0.001800  0.001712  0.001887   \n",
       "      3145728         0.002212  0.002137  0.002160  0.002241  0.002079   \n",
       "      3670016         0.002339  0.002300  0.002348  0.002328  0.002291   \n",
       "      4194304         0.002065  0.002076  0.002064  0.002013  0.002053   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "chr22 46661632        0.001992  0.002012  0.002025  0.002113  0.001952   \n",
       "      47185920        0.001951  0.001886  0.001928  0.001988  0.001941   \n",
       "      47710208        0.002395  0.002377  0.002320  0.002406  0.002419   \n",
       "      48234496        0.002998  0.003083  0.002923  0.002916  0.002975   \n",
       "      48758784        0.002848  0.002882  0.002728  0.002764  0.002769   \n",
       "\n",
       "                                                                        \n",
       "                                                                        \n",
       "                                                                        \n",
       "                       HG04039   HG04100   HG04107   HG04210   HG04229  \n",
       "chr   windowStartPos                                                    \n",
       "chr1  1048576         0.001788  0.001765  0.001911  0.001840  0.001890  \n",
       "      1572864         0.001681  0.001730  0.001588  0.001713  0.001741  \n",
       "      3145728         0.002160  0.002184  0.002215  0.002143  0.002134  \n",
       "      3670016         0.002336  0.002291  0.002300  0.002388  0.002237  \n",
       "      4194304         0.002152  0.002058  0.002006  0.002014  0.002097  \n",
       "...                        ...       ...       ...       ...       ...  \n",
       "chr22 46661632        0.002083  0.002062  0.002024  0.002048  0.002056  \n",
       "      47185920        0.001966  0.001973  0.002007  0.002027  0.002043  \n",
       "      47710208        0.002425  0.002358  0.002425  0.002465  0.002489  \n",
       "      48234496        0.003082  0.002967  0.002978  0.003037  0.003124  \n",
       "      48758784        0.002750  0.002879  0.002802  0.002766  0.002911  \n",
       "\n",
       "[4873 rows x 2457 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ancestor</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>super_pop</th>\n",
       "      <th>sub_pop</th>\n",
       "      <th>sex</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1KG</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AFR_ACB_female_HG01880</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998409</td>\n",
       "      <td>AFR</td>\n",
       "      <td>ACB</td>\n",
       "      <td>female</td>\n",
       "      <td>HG01880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFR_ACB_female_HG01883</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998386</td>\n",
       "      <td>AFR</td>\n",
       "      <td>ACB</td>\n",
       "      <td>female</td>\n",
       "      <td>HG01883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFR_ACB_female_HG01886</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998377</td>\n",
       "      <td>AFR</td>\n",
       "      <td>ACB</td>\n",
       "      <td>female</td>\n",
       "      <td>HG01886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFR_ACB_female_HG01889</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998372</td>\n",
       "      <td>AFR</td>\n",
       "      <td>ACB</td>\n",
       "      <td>female</td>\n",
       "      <td>HG01889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFR_ACB_female_HG01894</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998384</td>\n",
       "      <td>AFR</td>\n",
       "      <td>ACB</td>\n",
       "      <td>female</td>\n",
       "      <td>HG01894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAS_STU_male_HG04039</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998478</td>\n",
       "      <td>SAS</td>\n",
       "      <td>STU</td>\n",
       "      <td>male</td>\n",
       "      <td>HG04039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAS_STU_male_HG04100</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998473</td>\n",
       "      <td>SAS</td>\n",
       "      <td>STU</td>\n",
       "      <td>male</td>\n",
       "      <td>HG04100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAS_STU_male_HG04107</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998479</td>\n",
       "      <td>SAS</td>\n",
       "      <td>STU</td>\n",
       "      <td>male</td>\n",
       "      <td>HG04107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAS_STU_male_HG04210</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998473</td>\n",
       "      <td>SAS</td>\n",
       "      <td>STU</td>\n",
       "      <td>male</td>\n",
       "      <td>HG04210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAS_STU_male_HG04229</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998473</td>\n",
       "      <td>SAS</td>\n",
       "      <td>STU</td>\n",
       "      <td>male</td>\n",
       "      <td>HG04229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2456 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ancestor    seq_id super_pop sub_pop     sex  \\\n",
       "1KG                                                                            \n",
       "AFR_ACB_female_HG01880  hsmrca_ancestral  0.998409       AFR     ACB  female   \n",
       "AFR_ACB_female_HG01883  hsmrca_ancestral  0.998386       AFR     ACB  female   \n",
       "AFR_ACB_female_HG01886  hsmrca_ancestral  0.998377       AFR     ACB  female   \n",
       "AFR_ACB_female_HG01889  hsmrca_ancestral  0.998372       AFR     ACB  female   \n",
       "AFR_ACB_female_HG01894  hsmrca_ancestral  0.998384       AFR     ACB  female   \n",
       "...                                  ...       ...       ...     ...     ...   \n",
       "SAS_STU_male_HG04039    hsmrca_ancestral  0.998478       SAS     STU    male   \n",
       "SAS_STU_male_HG04100    hsmrca_ancestral  0.998473       SAS     STU    male   \n",
       "SAS_STU_male_HG04107    hsmrca_ancestral  0.998479       SAS     STU    male   \n",
       "SAS_STU_male_HG04210    hsmrca_ancestral  0.998473       SAS     STU    male   \n",
       "SAS_STU_male_HG04229    hsmrca_ancestral  0.998473       SAS     STU    male   \n",
       "\n",
       "                             id  \n",
       "1KG                              \n",
       "AFR_ACB_female_HG01880  HG01880  \n",
       "AFR_ACB_female_HG01883  HG01883  \n",
       "AFR_ACB_female_HG01886  HG01886  \n",
       "AFR_ACB_female_HG01889  HG01889  \n",
       "AFR_ACB_female_HG01894  HG01894  \n",
       "...                         ...  \n",
       "SAS_STU_male_HG04039    HG04039  \n",
       "SAS_STU_male_HG04100    HG04100  \n",
       "SAS_STU_male_HG04107    HG04107  \n",
       "SAS_STU_male_HG04210    HG04210  \n",
       "SAS_STU_male_HG04229    HG04229  \n",
       "\n",
       "[2456 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genome_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_wide = pd.read_table('%s/comp_tables/anc_genomewide_averages_seq.csv' % RESULTS_PATH,\n",
    "sep=',', index_col=0)\n",
    "genome_wide['seq_diff'] = 1-genome_wide['seq_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ancestor</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>super_pop</th>\n",
       "      <th>sub_pop</th>\n",
       "      <th>sex</th>\n",
       "      <th>id</th>\n",
       "      <th>seq_diff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1KG</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AFR_ACB_female_HG01880</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998409</td>\n",
       "      <td>AFR</td>\n",
       "      <td>ACB</td>\n",
       "      <td>female</td>\n",
       "      <td>HG01880</td>\n",
       "      <td>0.001591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFR_ACB_female_HG01883</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998386</td>\n",
       "      <td>AFR</td>\n",
       "      <td>ACB</td>\n",
       "      <td>female</td>\n",
       "      <td>HG01883</td>\n",
       "      <td>0.001614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFR_ACB_female_HG01886</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998377</td>\n",
       "      <td>AFR</td>\n",
       "      <td>ACB</td>\n",
       "      <td>female</td>\n",
       "      <td>HG01886</td>\n",
       "      <td>0.001623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFR_ACB_female_HG01889</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998372</td>\n",
       "      <td>AFR</td>\n",
       "      <td>ACB</td>\n",
       "      <td>female</td>\n",
       "      <td>HG01889</td>\n",
       "      <td>0.001628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFR_ACB_female_HG01894</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998384</td>\n",
       "      <td>AFR</td>\n",
       "      <td>ACB</td>\n",
       "      <td>female</td>\n",
       "      <td>HG01894</td>\n",
       "      <td>0.001616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAS_STU_male_HG04039</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998478</td>\n",
       "      <td>SAS</td>\n",
       "      <td>STU</td>\n",
       "      <td>male</td>\n",
       "      <td>HG04039</td>\n",
       "      <td>0.001522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAS_STU_male_HG04100</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998473</td>\n",
       "      <td>SAS</td>\n",
       "      <td>STU</td>\n",
       "      <td>male</td>\n",
       "      <td>HG04100</td>\n",
       "      <td>0.001527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAS_STU_male_HG04107</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998479</td>\n",
       "      <td>SAS</td>\n",
       "      <td>STU</td>\n",
       "      <td>male</td>\n",
       "      <td>HG04107</td>\n",
       "      <td>0.001521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAS_STU_male_HG04210</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998473</td>\n",
       "      <td>SAS</td>\n",
       "      <td>STU</td>\n",
       "      <td>male</td>\n",
       "      <td>HG04210</td>\n",
       "      <td>0.001527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAS_STU_male_HG04229</th>\n",
       "      <td>hsmrca_ancestral</td>\n",
       "      <td>0.998473</td>\n",
       "      <td>SAS</td>\n",
       "      <td>STU</td>\n",
       "      <td>male</td>\n",
       "      <td>HG04229</td>\n",
       "      <td>0.001527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2456 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ancestor    seq_id super_pop sub_pop     sex  \\\n",
       "1KG                                                                            \n",
       "AFR_ACB_female_HG01880  hsmrca_ancestral  0.998409       AFR     ACB  female   \n",
       "AFR_ACB_female_HG01883  hsmrca_ancestral  0.998386       AFR     ACB  female   \n",
       "AFR_ACB_female_HG01886  hsmrca_ancestral  0.998377       AFR     ACB  female   \n",
       "AFR_ACB_female_HG01889  hsmrca_ancestral  0.998372       AFR     ACB  female   \n",
       "AFR_ACB_female_HG01894  hsmrca_ancestral  0.998384       AFR     ACB  female   \n",
       "...                                  ...       ...       ...     ...     ...   \n",
       "SAS_STU_male_HG04039    hsmrca_ancestral  0.998478       SAS     STU    male   \n",
       "SAS_STU_male_HG04100    hsmrca_ancestral  0.998473       SAS     STU    male   \n",
       "SAS_STU_male_HG04107    hsmrca_ancestral  0.998479       SAS     STU    male   \n",
       "SAS_STU_male_HG04210    hsmrca_ancestral  0.998473       SAS     STU    male   \n",
       "SAS_STU_male_HG04229    hsmrca_ancestral  0.998473       SAS     STU    male   \n",
       "\n",
       "                             id  seq_diff  \n",
       "1KG                                        \n",
       "AFR_ACB_female_HG01880  HG01880  0.001591  \n",
       "AFR_ACB_female_HG01883  HG01883  0.001614  \n",
       "AFR_ACB_female_HG01886  HG01886  0.001623  \n",
       "AFR_ACB_female_HG01889  HG01889  0.001628  \n",
       "AFR_ACB_female_HG01894  HG01894  0.001616  \n",
       "...                         ...       ...  \n",
       "SAS_STU_male_HG04039    HG04039  0.001522  \n",
       "SAS_STU_male_HG04100    HG04100  0.001527  \n",
       "SAS_STU_male_HG04107    HG04107  0.001521  \n",
       "SAS_STU_male_HG04210    HG04210  0.001527  \n",
       "SAS_STU_male_HG04229    HG04229  0.001527  \n",
       "\n",
       "[2456 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genome_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indivs_avgs = pd.DataFrame(index = seq_diff.index)\n",
    "all_indivs_avgs['seq_diff'] = seq_diff.mean(axis=1)\n",
    "all_indivs_avgs['3d_div'] = anc_div.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x460 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,4.6))   \n",
    "_ = sns.scatterplot(x='seq_diff', y='3d_div', data=all_indivs_avgs, s=12,alpha=0.5)\n",
    "_ = sns.regplot(data=all_indivs_avgs, y=\"3d_div\", x=\"seq_diff\", scatter=False, ax=ax,color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(all_indivs_avgs.loc['chr1'].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = dict.fromkeys(windows_to_keep.get_level_values(0).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chrm in chunks.keys():\n",
    "    chunks[chrm] = list(all_indivs_avgs.loc[chrm].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wynton/group/capra/projects/modern_human_3Dgenome/data/reference'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH+'/reference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( chunks, open( \"%s/reference/genome_chunks_dict.p\" % DATA_PATH, \"wb\" ) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshopping the expected divergence code from Evonne"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_inFileLoc(indiv, chrm):\n",
    "    if indiv == 'hsmrca_ancestral':\n",
    "        seq_dir = os.path.join(FASTA_PATH, \"human_archaic_ancestor\")\n",
    "        file_path = os.path.join(seq_dir, (\"human_archaic_ancestor_in_hg38_%s.fasta\" % chrm))\n",
    "    else:\n",
    "        seq_dir = os.path.join(FASTA_PATH, \"1KG\")\n",
    "        pop = indiv.split('_')[0]\n",
    "        iid = indiv.split('_')[-1]\n",
    "        file_path = os.path.join(seq_dir, (\"%s/%s/%s_%s_hg38_full.fa\" % (pop, indiv, chrm, iid)))\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "0\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "2\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "3\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "4\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "5\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "6\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "7\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "8\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "9\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "10\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "11\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "12\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "13\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "14\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "15\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "16\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "17\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "18\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "19\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "20\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "21\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "22\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "23\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "24\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "25\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "26\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "27\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "28\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "29\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "30\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "31\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "32\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "33\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "34\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "35\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "36\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "37\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "38\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "39\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "40\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "41\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "42\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "43\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "44\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "45\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "46\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "47\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "48\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "49\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "50\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "51\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "52\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "53\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "54\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "55\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "56\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "57\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "58\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "59\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "60\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "61\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "62\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "63\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "64\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "65\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "66\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "67\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "68\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "69\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "70\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "71\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "72\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "73\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "74\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "75\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "76\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "77\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "78\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "79\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "80\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "81\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "82\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "83\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "84\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "85\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "86\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "87\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "88\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "89\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "90\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "91\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "92\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "93\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "94\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "95\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "96\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "97\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "98\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "99\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "0\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "2\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "3\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "4\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "5\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "6\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "7\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "8\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "9\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "10\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "11\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "12\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "13\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "14\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "15\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "16\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "17\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "18\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "19\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "20\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "21\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "22\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "23\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "24\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "25\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "26\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "27\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "28\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "29\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "30\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "31\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "32\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "33\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "34\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "35\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "36\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "37\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "38\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "39\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "40\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "41\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "42\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "43\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "44\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "45\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "46\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "47\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "48\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "49\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "50\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "51\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "52\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "53\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "54\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "55\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "56\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "57\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "58\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "59\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "60\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "61\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "62\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "63\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "64\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "65\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "66\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "67\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "68\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "69\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "70\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "71\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "72\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "73\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "74\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "75\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "76\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "77\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "78\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "79\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "80\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "81\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "82\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "83\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "84\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "85\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "86\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "87\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "88\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "89\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "90\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "91\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "92\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "93\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "94\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "95\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "96\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "97\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "98\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "99\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "0\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "2\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "3\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "4\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "5\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "6\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "7\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "8\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "9\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "10\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "11\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "12\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "13\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "14\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "15\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "16\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "17\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "18\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "19\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "20\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "21\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "22\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "23\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "24\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "25\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "26\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "27\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "28\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "29\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "30\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "31\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "32\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "33\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "34\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "35\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "36\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "37\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "38\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "39\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "40\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "41\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m indexes \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(indexes,\u001b[39mlen\u001b[39m(v)) \u001b[39m#randomly sample\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indexes:\n\u001b[0;32m---> 67\u001b[0m     anc_ignore \u001b[39m=\u001b[39m anc_ignore[:i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mN\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m anc_ignore[i\u001b[39m+\u001b[39;49m\u001b[39m2\u001b[39;49m:] \u001b[39m# mask the original sequence with an N so it will not try to resample this part of the sequence\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m i,m \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(indexes,v):\n\u001b[1;32m     69\u001b[0m     random_mutated \u001b[39m=\u001b[39m random_mutated[:i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m m \u001b[39m+\u001b[39m random_mutated[i\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m:] \u001b[39m# mutate trinucleotide in new posision\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define variables to store individual IDs for ancestral and modern samples\n",
    "anc_indiv = 'hsmrca_ancestral'\n",
    "mod_indiv = 'AFR_ESN_female_HG03105'\n",
    "chrm='chr20'\n",
    "\n",
    "# Use the function find_inFileLoc to get the file path for the input files for the given chromosome\n",
    "in_file_loc_anc_indiv = find_inFileLoc(anc_indiv, chrm)\n",
    "in_file_loc_mod_indiv = find_inFileLoc(mod_indiv, chrm)\n",
    "\n",
    "# Open the fasta files for the ancestral and modern individuals\n",
    "anc_indiv_fasta_open = pysam.Fastafile(in_file_loc_anc_indiv)\n",
    "mod_indiv_fasta_open = pysam.Fastafile(in_file_loc_mod_indiv)\n",
    "\n",
    "# Create a new file to write the output to\n",
    "f = open(\"%s_empiricDist_test.tsv\" % chrm,\"w\")\n",
    "\n",
    "for start_loc in chunks[chrm]:\n",
    "    anc_indiv_seq = anc_indiv_fasta_open.fetch(chrm, start_loc, start_loc+2**20).upper()\n",
    "    mod_indiv_seq = mod_indiv_fasta_open.fetch(chrm, start_loc, start_loc+2**20).upper()\n",
    "    # Calculate the number of differences between the ancestral and modern sequences\n",
    "    diff = [1 if anc != mod else 0 for anc,mod in zip(anc_indiv_seq,mod_indiv_seq)]\n",
    "    diff_num = sum(diff)\n",
    "    anc_pred  = runAkitaPreds(anc_indiv_seq)[:,:,0][0]\n",
    "    \n",
    "    diffs = dict()\n",
    "\n",
    "    for i in range(len(anc_indiv_seq)):\n",
    "        if diff[i] == 1: # make sure a difference is present\n",
    "            if i == 0: # If the base is at the start of the sequence, the trinucleotide context is defined as the base itself and the following base\n",
    "                if (anc_indiv_seq[i+1] == 'N'):\n",
    "                    triNuc = anc_indiv_seq[i]\n",
    "                else:\n",
    "                    triNuc = \"\".join([anc_indiv_seq[i],anc_indiv_seq[i+1]])\n",
    "            elif i == len(anc_indiv_seq)-1: #  # If the base is at the end of the sequence, the trinucleotide context is defined as the base itself and the preceding base\n",
    "                if (anc_indiv_seq[i-1] == \"N\"):\n",
    "                    triNuc = anc_indiv_seq[i]\n",
    "                else:\n",
    "                    triNuc = \"\".join([anc_indiv_seq[i-1],anc_indiv_seq[i]])\n",
    "            else: # Otherwise, the trinucleotide context is defined as the three nucleotides immediately upstream and downstream of the base\n",
    "                if (anc_indiv_seq[i-1] == 'N') and (anc_indiv_seq[i+1] == 'N'):\n",
    "                    triNuc = anc_indiv_seq[i]\n",
    "                elif (anc_indiv_seq[i-1] == 'N'):\n",
    "                    triNuc = \"\".join([anc_indiv_seq[i],anc_indiv_seq[i+1]])\n",
    "                elif (anc_indiv_seq[i+1] == 'N'):\n",
    "                    triNuc = \"\".join([anc_indiv_seq[i-1],anc_indiv_seq[i]])\n",
    "                else:\n",
    "                    triNuc = \"\".join([anc_indiv_seq[i-1],anc_indiv_seq[i],anc_indiv_seq[i+1]])\n",
    "                    \n",
    "            # Add the archaic sequence at this position to the dictionary for this trinucleotide context\n",
    "            if triNuc not in diffs:\n",
    "                diffs[triNuc] = [mod_indiv_seq[i]]\n",
    "            else:     # If the trinucleotide context is already in the dictionary, append the archaic sequence at this position to the list for this trinucleotide context\n",
    "                diffs[triNuc].append(mod_indiv_seq[i])\n",
    "\n",
    "    empirical_dist = []\n",
    "    # Repeat the following process 100 times to create a null distribution of differences\n",
    "    for iter in range(100): \n",
    "        print(iter,flush=True)\n",
    "        # Create a randomly mutated version of the African individual\n",
    "        random_mutated = anc_indiv_seq\n",
    "        anc_ignore = anc_indiv_seq\n",
    "\n",
    "        for k,v in diffs.items(): # For each of the differences\n",
    "                indexes = [m.start() for m in re.finditer(k, anc_ignore)] #find the sequence\n",
    "                indexes = random.sample(indexes,len(v)) #randomly sample\n",
    "                for i in indexes:\n",
    "                    anc_ignore = anc_ignore[:i+1] + \"N\" + anc_ignore[i+2:] # mask the original sequence with an N so it will not try to resample this part of the sequence\n",
    "                for i,m in zip(indexes,v):\n",
    "                    random_mutated = random_mutated[:i+1] + m + random_mutated[i+2:] # mutate trinucleotide in new posision\n",
    "        \n",
    "        random_pred  = runAkitaPreds(random_mutated)[:,:,0][0] #run akita on the shuffled sequence\n",
    "        empirical_dist.append(1-stats.spearmanr(anc_pred, random_pred)[0]) # calculate the spearman on the shuffled sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.write(chrm + \"\\t\" + str(start_loc) + \"\\t\" + str(diff_num) + \"\\t\" + \"\\t\".join([str(x) for x in empirical_dist]) + \"\\n\")\n",
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "0\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "2\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "3\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "4\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "5\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "6\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "7\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "8\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "9\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "10\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "11\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "12\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "13\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "14\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "15\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "16\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "17\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "18\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "19\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "20\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "21\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "22\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "23\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "24\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "25\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "26\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "27\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "28\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "29\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "30\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "31\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "32\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "33\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "34\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "35\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "36\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "37\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "38\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "39\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "40\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "41\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "42\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "43\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "44\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "45\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "46\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "47\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "48\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "49\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "50\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "51\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "52\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "53\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "54\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "55\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "56\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "57\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "58\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "59\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "60\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "61\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "62\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "63\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "64\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "65\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "66\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "67\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "68\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "69\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "70\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "71\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "72\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "73\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "74\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "75\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "76\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "77\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "78\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "79\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "80\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "81\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "82\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "83\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "84\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "85\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "86\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "87\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "88\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "89\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "90\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "91\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "92\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "93\n",
      "run predictions\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "94\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "95\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "96\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "97\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "98\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "99\n",
      "run predictions\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Define variables to store individual IDs for ancestral and modern samples\n",
    "anc_indiv = 'hsmrca_ancestral'\n",
    "mod_indiv = 'AFR_ESN_female_HG03105'\n",
    "chrm='chr20'\n",
    "start_loc = chunks[chrm][0]\n",
    "# Use the function find_inFileLoc to get the file path for the input files for the given chromosome\n",
    "in_file_loc_anc_indiv = find_inFileLoc(anc_indiv, chrm)\n",
    "in_file_loc_mod_indiv = find_inFileLoc(mod_indiv, chrm)\n",
    "\n",
    "# Open the fasta files for the ancestral and modern individuals\n",
    "anc_indiv_fasta_open = pysam.Fastafile(in_file_loc_anc_indiv)\n",
    "mod_indiv_fasta_open = pysam.Fastafile(in_file_loc_mod_indiv)\n",
    "\n",
    "# Create a new file to write the output to\n",
    "f = open(\"%s_empiricDist_test.tsv\" % chrm,\"w\")\n",
    "\n",
    "anc_indiv_seq = anc_indiv_fasta_open.fetch(chrm, start_loc, start_loc+2**20).upper()\n",
    "mod_indiv_seq = mod_indiv_fasta_open.fetch(chrm, start_loc, start_loc+2**20).upper()\n",
    "# Calculate the number of differences between the ancestral and modern sequences\n",
    "diff = [1 if anc != mod else 0 for anc,mod in zip(anc_indiv_seq,mod_indiv_seq)]\n",
    "diff_num = sum(diff)\n",
    "anc_pred  = runAkitaPreds(anc_indiv_seq)[:,:,0][0]\n",
    "\n",
    "diffs = dict()\n",
    "\n",
    "for i in range(len(anc_indiv_seq)):\n",
    "    if diff[i] == 1: # make sure a difference is present\n",
    "        if i == 0: # If the base is at the start of the sequence, the trinucleotide context is defined as the base itself and the following base\n",
    "            if (anc_indiv_seq[i+1] == 'N'):\n",
    "                triNuc = anc_indiv_seq[i]\n",
    "            else:\n",
    "                triNuc = \"\".join([anc_indiv_seq[i],anc_indiv_seq[i+1]])\n",
    "        elif i == len(anc_indiv_seq)-1: #  # If the base is at the end of the sequence, the trinucleotide context is defined as the base itself and the preceding base\n",
    "            if (anc_indiv_seq[i-1] == \"N\"):\n",
    "                triNuc = anc_indiv_seq[i]\n",
    "            else:\n",
    "                triNuc = \"\".join([anc_indiv_seq[i-1],anc_indiv_seq[i]])\n",
    "        else: # Otherwise, the trinucleotide context is defined as the three nucleotides immediately upstream and downstream of the base\n",
    "            if (anc_indiv_seq[i-1] == 'N') and (anc_indiv_seq[i+1] == 'N'):\n",
    "                triNuc = anc_indiv_seq[i]\n",
    "            elif (anc_indiv_seq[i-1] == 'N'):\n",
    "                triNuc = \"\".join([anc_indiv_seq[i],anc_indiv_seq[i+1]])\n",
    "            elif (anc_indiv_seq[i+1] == 'N'):\n",
    "                triNuc = \"\".join([anc_indiv_seq[i-1],anc_indiv_seq[i]])\n",
    "            else:\n",
    "                triNuc = \"\".join([anc_indiv_seq[i-1],anc_indiv_seq[i],anc_indiv_seq[i+1]])\n",
    "                \n",
    "        # Add the archaic sequence at this position to the dictionary for this trinucleotide context\n",
    "        if triNuc not in diffs:\n",
    "            diffs[triNuc] = [mod_indiv_seq[i]]\n",
    "        else:     # If the trinucleotide context is already in the dictionary, append the archaic sequence at this position to the list for this trinucleotide context\n",
    "            diffs[triNuc].append(mod_indiv_seq[i])\n",
    "\n",
    "empirical_dist = []\n",
    "# Repeat the following process 100 times to create a null distribution of differences\n",
    "for iter in range(100): \n",
    "    print(iter,flush=True)\n",
    "    # Create a randomly mutated version of the African individual\n",
    "    random_mutated = anc_indiv_seq\n",
    "    anc_ignore = anc_indiv_seq\n",
    "\n",
    "    for k,v in diffs.items(): # For each of the differences\n",
    "            indexes = [m.start() for m in re.finditer(k, anc_ignore)] #find the sequence\n",
    "            indexes = random.sample(indexes,len(v)) #randomly sample\n",
    "            for i in indexes:\n",
    "                anc_ignore = anc_ignore[:i+1] + \"N\" + anc_ignore[i+2:] # mask the original sequence with an N so it will not try to resample this part of the sequence\n",
    "            for i,m in zip(indexes,v):\n",
    "                random_mutated = random_mutated[:i+1] + m + random_mutated[i+2:] # mutate trinucleotide in new posision\n",
    "    \n",
    "    random_pred  = runAkitaPreds(random_mutated)[:,:,0][0] #run akita on the shuffled sequence\n",
    "    empirical_dist.append(1-stats.spearmanr(anc_pred, random_pred)[0]) # calculate the spearman on the shuffled sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks['chr1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.31666666666667"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(418*5.5)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.612152777777776"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((len(windows_to_keep)*5.5)/60)/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empirical_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables to store individual IDs for African and archaic samples\n",
    "afr_indiv = 'AFR_ESN_female_HG03105'\n",
    "arc_indiv = 'vindija'\n",
    "\n",
    "# Use the function find_inFileLoc to get the file path for the input files for the given chromosome\n",
    "in_file_loc_afr_indiv = find_inFileLoc(afr_indiv, chrm)\n",
    "in_file_loc_arc_indiv = find_inFileLoc(arc_indiv, chrm)\n",
    "\n",
    "# Open the fasta files for the African and archaic individuals\n",
    "afr_indiv_fasta_open = pysam.Fastafile(in_file_loc_afr_indiv)\n",
    "arc_indiv_fasta_open = pysam.Fastafile(in_file_loc_arc_indiv)\n",
    "\n",
    "# Open the fasta file for the masked reference sequence\n",
    "mask_fasta_open = pysam.Fastafile('/gpfs51/dors2/capra_lab/users/rinkerd/projects/3DNeand/data/genomes/masked_hg19_reference/%s_hg19_archaic.masked.fa' % chrm) #for the masked\n",
    "# Open the fasta file for the unmasked reference sequence\n",
    "human19_fasta_open = pysam.Fastafile('/dors/capra_lab/data/dna/human/hg19/%s.fa' % chrm)\n",
    "\n",
    "# Create a new file to write the output to\n",
    "f = open(\"%s_empiricDist.tsv3\" % chrm,\"w\")\n",
    "\n",
    "# Loop over the start locations in the chunks dictionary for the current chromosome (File is parallelized)\n",
    "for start_loc in chunks[chrm]:\n",
    "#for start_loc in [pos]:\n",
    "#extract sequences\n",
    "    # Get the sequences for the African and archaic individuals, masked reference sequence and reference sequence\n",
    "    afr_indiv_seq = afr_indiv_fasta_open.fetch(chrm, start_loc, start_loc+2**20).upper()\n",
    "    arc_indiv_seq = arc_indiv_fasta_open.fetch(chrm, start_loc, start_loc+2**20).upper()\n",
    "    masked_seq = mask_fasta_open.fetch(chrm, start_loc, start_loc+2**20).upper() #for the masked \n",
    "    human19_seq = human19_fasta_open.fetch(chrm, start_loc, start_loc+2**20).upper()\n",
    "    \n",
    "    # Check if there are 'N's in the human19 sequence and skip to the next start location if there are\n",
    "    if 'N' in human19_seq:\n",
    "        print('Ns in %s:%s' % (chrm, start_loc),flush=True)\n",
    "        continue\n",
    "    \n",
    "    # Create a masked sequence for the individual by replacing 'N's with 'N' and all other bases with the corresponding base\n",
    "    afr_masked = \"\".join([\"N\" if m == \"N\" else \"N\" if afr == \"N\" else  \"N\" if arc == \"N\" else afr for m, afr, arc in zip(masked_seq, afr_indiv_seq, arc_indiv_seq)])\n",
    "    arc_masked = \"\".join([\"N\" if m == \"N\" else \"N\" if afr == \"N\" else  \"N\" if arc == \"N\" else arc for m, afr, arc in zip(masked_seq, afr_indiv_seq, arc_indiv_seq)])\n",
    "    \n",
    "    # Calculate the number of differences between the African and archaic masked sequences\n",
    "    diff = [1 if afr != arc else 0 for afr,arc in zip(afr_masked,arc_masked)]\n",
    "    diff_num = sum(diff)\n",
    "    \n",
    "    \n",
    "    # Create a filled sequence for the African individual by replacing 'N's with the corresponding base\n",
    "    afr_filled = \"\".join([r if m == \"N\" else r if s == \"N\" else s for r, m, s in zip(human19_seq, masked_seq, afr_masked)])\n",
    "    # Run akita prediction on filled sequence\n",
    "    afr_pred  = runAkitaPreds(afr_filled)[:,:,0][0]\n",
    "\n",
    "    diffs = dict()\n",
    "    # Loop over each position in the masked sequences and store the differences between the African and archaic sequences that match the trinucleotide content\n",
    "    for i in range(len(afr_masked)):\n",
    "        if diff[i] == 1: # make sure a difference is present\n",
    "            if i == 0: # If the base is at the start of the sequence, the trinucleotide context is defined as the base itself and the following base\n",
    "                if (afr_masked[i+1] == 'N'):\n",
    "                    triNuc = afr_masked[i]\n",
    "                else:\n",
    "                    triNuc = \"\".join([afr_masked[i],afr_masked[i+1]])\n",
    "            elif i == len(afr_masked)-1: #  # If the base is at the end of the sequence, the trinucleotide context is defined as the base itself and the preceding base\n",
    "                if (afr_masked[i-1] == \"N\"):\n",
    "                    triNuc = afr_masked[i]\n",
    "                else:\n",
    "                    triNuc = \"\".join([afr_masked[i-1],afr_masked[i]])\n",
    "            else:\n",
    "                # Otherwise, the trinucleotide context is defined as the three nucleotides immediately upstream and downstream of the base\n",
    "                if (afr_masked[i-1] == 'N') and (afr_masked[i+1] == 'N'):\n",
    "                    triNuc = afr_masked[i]\n",
    "                elif (afr_masked[i-1] == 'N'):\n",
    "                    triNuc = \"\".join([afr_masked[i],afr_masked[i+1]])\n",
    "                elif (afr_masked[i+1] == 'N'):\n",
    "                    triNuc = \"\".join([afr_masked[i-1],afr_masked[i]])\n",
    "                else:\n",
    "                    triNuc = \"\".join([afr_masked[i-1],afr_masked[i],afr_masked[i+1]])\n",
    "                    \n",
    "            # Add the archaic sequence at this position to the dictionary for this trinucleotide context\n",
    "            if triNuc not in diffs:\n",
    "                diffs[triNuc] = [arc_masked[i]]\n",
    "            else:     # If the trinucleotide context is already in the dictionary, append the archaic sequence at this position to the list for this trinucleotide context\n",
    "                diffs[triNuc].append(arc_masked[i])\n",
    "\n",
    "    empirical_dist = []\n",
    "    # Repeat the following process 100 times to create a null distribution of differences\n",
    "    for iter in range(100): \n",
    "        print(iter,flush=True)\n",
    "        \n",
    "        # Create a randomly mutated version of the African individual\n",
    "        random_mutated_masked = afr_masked\n",
    "        afr_masked_ignore = afr_masked\n",
    "\n",
    "        for k,v in diffs.items(): # For each of the differences\n",
    "            indexes = [m.start() for m in re.finditer(k, afr_masked_ignore)] #find the sequence\n",
    "            indexes = random.sample(indexes,len(v)) #randomly sample\n",
    "            for i in indexes:\n",
    "                afr_masked_ignore = afr_masked_ignore[:i+1] + \"N\" + afr_masked_ignore[i+2:] # mask the original sequence with an N so it will not try to resample this part of the sequence\n",
    "            for i,m in zip(indexes,v):\n",
    "                random_mutated_masked = random_mutated_masked[:i+1] + m + random_mutated_masked[i+2:] # mutate trinucleotide in new posision\n",
    "\n",
    "        random_mutated_filled = \"\".join([r if m == \"N\" else r if s == \"N\" else s for r, m, s in zip(human19_seq, masked_seq, random_mutated_masked)])\n",
    "    \n",
    "        random_pred  = runAkitaPreds(random_mutated_filled)[:,:,0][0] #run akita on the shuffled sequence\n",
    "        empirical_dist.append(stats.spearmanr(afr_pred, random_pred)[0]) # calculate the spearman on the shuffled sequence\n",
    "    f.write(chrm + \"\\t\" + str(start_loc) + \"\\t\" + str(diff_num) + \"\\t\" + \"\\t\".join([str(x) for x in empirical_dist]) + \"\\n\")\n",
    "    f.flush()\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f46e2c836e5bf05537a649933a2b4fa95e1eca7738f51dce3dfe321ed30c13c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
